{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55d9ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules for data manipulation, machine learning models, and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd  # Pandas library for data manipulation and analysis\n",
    "import numpy as np  # Numpy library for numerical computations\n",
    "from sklearn.svm import SVC  # Support Vector Classification from Scikit-learn SVM module\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model from Scikit-learn\n",
    "from sklearn.model_selection import train_test_split  # Function to split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "771e4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "cc_df = pd.read_excel('CCPP.xlsx')  # Load the Combined Cycle Power Plant dataset from an Excel file.\n",
    "# This dataset is expected to contain multiple columns where the first four are considered as features (inputs) for the model,\n",
    "# and the fifth column is the target variable (output) the model aims to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8dcfc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# The next steps involve preparing the data for machine learning by selecting the appropriate features and target variable,\n",
    "# and then converting them into a format (NumPy arrays) that is compatible with machine learning algorithms.\n",
    "\n",
    "X = cc_df.iloc[:, :4].values  # Select all rows and the first four columns as features.\n",
    "# .iloc[:, :4] is used for integer-location based indexing to select the features (inputs) from the dataset.\n",
    "# The selection includes all rows (:) and the first four columns (:4) which are then converted into a NumPy array using .values.\n",
    "# These features are the inputs that the machine learning model will use to make predictions.\n",
    "\n",
    "y = cc_df.iloc[:, 4].values  # Select all rows and the fifth column as the target variable.\n",
    "# Similarly, .iloc[:, 4] selects all rows (:) and specifically the fifth column (4) as the target variable.\n",
    "# The target variable is converted into a NumPy array using .values, which the model will be trained to predict based on the inputs (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02887ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # Split the features and target variable into training (70%) and testing (30%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dcf3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler for feature scaling\n",
    "sc_X = StandardScaler()  # Create an instance of StandardScaler for the features\n",
    "sc_y = StandardScaler()  # Create an instance of StandardScaler for the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "413de2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The StandardScaler in Python, provided by the scikit-learn library, is a preprocessing utility that is used to standardize\n",
    "# the features of your dataset. Standardization refers to the process of scaling your data so that it has a mean of 0 and a standard\n",
    "# deviation of 1. This is achieved by subtracting the mean value of each feature from the dataset and then dividing the feature values\n",
    "# by their respective standard deviations. Standardizing the features is important because it ensures that each feature contributes\n",
    "# equally to the analysis, which is particularly useful for machine learning models that are sensitive to the scale of the input data,\n",
    "# such as Support Vector Machines (SVMs) and k-nearest neighbors (KNN). By using StandardScaler, you can improve the convergence of\n",
    "# stochastic gradient descent algorithms and the overall performance of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8a0981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features to improve model performance\n",
    "# This step is crucial for algorithms sensitive to the scale of data, such as SVM, k-NN, and PCA.\n",
    "\n",
    "# Initialize sc_X with a scaler object (e.g., StandardScaler or MinMaxScaler) before this line.\n",
    "# The choice of scaler depends on the desired scaling strategy:\n",
    "# - StandardScaler scales data to have mean = 0 and variance = 1.\n",
    "# - MinMaxScaler scales each feature to a specified range, typically [0, 1].\n",
    "\n",
    "X = sc_X.fit_transform(X)  # Fit to data, then transform it for the features.\n",
    "# The fit_transform method performs two operations:\n",
    "# 1. Fit: Calculate the scaling parameters (mean and std for StandardScaler; min and max for MinMaxScaler) based on the data.\n",
    "#    This ensures the scaling is tailored to the dataset's specific features.\n",
    "# 2. Transform: Apply the scaling transformation using the calculated parameters, adjusting the scale of the data accordingly.\n",
    "\n",
    "# The transformed dataset is then reassigned to X, replacing the original dataset.\n",
    "# This practice maintains simplicity in variable naming but note that the original scale of the data is not preserved.\n",
    "# If you need to reverse the transformation or access the original data later, keep a copy of the original dataset or the scaler object.\n",
    "\n",
    "# Scaling is a best practice for many machine learning algorithms, especially those calculating distances between data points\n",
    "# or assuming data is centered around zero. It helps in achieving better performance and more stable convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbc365f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the target variable is in the appropriate format and scale for machine learning algorithms\n",
    "\n",
    "# Convert the target variable to a NumPy array for uniformity and compatibility with scikit-learn methods.\n",
    "y = np.array(y)  # Ensure y is a numpy array to enable reshaping.\n",
    "\n",
    "# Reshape the target variable to have a single row.\n",
    "# This step is necessary for the scaling process, especially when the scaler expects a two-dimensional input.\n",
    "# The reshape method is used with (1, -1) to indicate that we want one row, with the number of columns automatically determined based on the length of y.\n",
    "y = np.reshape(y, (1, -1))  # Reshape y to make it suitable for scaling (1 row, as many columns as necessary).\n",
    "\n",
    "# Scale the target variable using a predefined scaler object (e.g., sc_y, which could be an instance of StandardScaler or MinMaxScaler).\n",
    "# The fit_transform method first calculates the scaling parameters based on the data in y (fit),\n",
    "# and then applies the scaling transformation (transform).\n",
    "# This step ensures the target variable is on a suitable scale for the machine learning algorithm, improving model performance.\n",
    "y = sc_y.fit_transform(y)  # Fit to data, then transform it for the target variable.\n",
    "\n",
    "# After scaling, the target variable is in a two-dimensional array form due to the earlier reshaping.\n",
    "# For many machine learning algorithms and further processing, we need to revert it back to its original shape.\n",
    "# The flatten method is used to collapse the array into one dimension, achieving the desired format for modeling.\n",
    "y = y.flatten()  # Flatten the array to revert it back to the original shape after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82c4fc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Regression (SVR) model\n",
    "from sklearn.svm import SVR  # Import SVR class from scikit-learn's svm module.\n",
    "# SVR is a type of Support Vector Machine (SVM) that is used for regression tasks, which predict a continuous value.\n",
    "\n",
    "regressor = SVR(kernel='rbf')  \n",
    "# Instantiate the SVR model with the Radial Basis Function (RBF) kernel.\n",
    "# The RBF kernel is a popular choice for SVR (and SVM classification) because it can model complex, non-linear relationships\n",
    "# between the features and the target variable. The kernel choice significantly affects the model's performance,\n",
    "# and RBF is known for its flexibility and ability to handle non-linear data.\n",
    "\n",
    "regressor.fit(X, y)  \n",
    "# Fit the SVR model to the scaled features (X) and target variable (y).\n",
    "# This step involves the model learning from the data by finding the hyperplane (or set of hyperplanes in higher-dimensional space)\n",
    "# that best fits the data points. The fitting process adjusts the model parameters to minimize the error between\n",
    "# the predicted values and the actual values in the dataset (y).\n",
    "# The 'fit' method is a critical step where the model 'learns' from the data, making it capable of making predictions.\n",
    "\n",
    "# Note: It's important that the features (X) are scaled before fitting the model, especially for algorithms like SVR,\n",
    "# which are sensitive to the scale of the input data. Scaling ensures that all features contribute equally to the model's training process,\n",
    "# improving performance and leading to more accurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
